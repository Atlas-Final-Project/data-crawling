# -*- coding: utf-8 -*-
"""
AP News í¬ë¡¤ëŸ¬
"""

import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from typing import List, Dict, Any, Optional
from .base_news_crawler import BaseNewsCrawler


class APNewsCrawler(BaseNewsCrawler):
    """AP News í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_file: str = "crawler_config.json"):
        super().__init__(config_file)
        self.source_name = "AP News"
        self.base_url = self.config.get("sources", {}).get("ap", {}).get("base_url", "")
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }        
        self.request_delay = 2  # ìš”ì²­ ê°„ ê¸°ë³¸ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.max_delay = 30  # ìµœëŒ€ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.current_delay = self.request_delay  # í˜„ì¬ ì§€ì—° ì‹œê°„
        self.retry_count = 0  # ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜
    
    def _adjust_delay(self, success: bool = True):
        """ìš”ì²­ ì„±ê³µ/ì‹¤íŒ¨ì— ë”°ë¥¸ ì§€ì—° ì‹œê°„ ì¡°ì ˆ (ì§€ìˆ˜ ë°±ì˜¤í”„)"""
        if success:
            # ì„±ê³µ ì‹œ ì§€ì—° ì‹œê°„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¦¬ì…‹
            self.current_delay = self.request_delay
            self.retry_count = 0
        else:
            # ì‹¤íŒ¨ ì‹œ ì§€ì—° ì‹œê°„ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€
            self.retry_count += 1
            self.current_delay = min(self.request_delay * (2 ** self.retry_count), self.max_delay)
            print(f"âš ï¸ {self.source_name}: ì§€ì—° ì‹œê°„ ì¡°ì ˆ - {self.current_delay}ì´ˆ ëŒ€ê¸°")
    
    def extract_article_links(self, limit: int = 50) -> List[str]:
        """AP ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (429 ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        try:
            response = requests.get(self.base_url, headers=self.headers, timeout=10)
            
            # 429 ì—ëŸ¬ ì²´í¬
            if response.status_code == 429:
                print(f"âš ï¸ {self.source_name}: 429 Too Many Requests ë°œìƒ")
                raise requests.exceptions.HTTPError("429 Too Many Requests")
            
            response.raise_for_status()
            
            # ê°„ë‹¨í•œ ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ë§í¬ ì¶”ì¶œ
            article_links = []
            
            # /article/ íŒ¨í„´ì´ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            pattern = r'href=["\']([^"\']*?/article/[^"\']*?)["\']'
            matches = re.findall(pattern, response.text)
            
            for href in matches:
                if href.startswith('/'):
                    href = urljoin("https://apnews.com", href)
                if href not in article_links:
                    article_links.append(href)
                    if len(article_links) >= limit:
                        break
            
            return article_links
            
        except requests.exceptions.HTTPError as e:
            if "429" in str(e):
                print(f"âŒ {self.source_name}: ìš”ì²­ ì œí•œì— ê±¸ë ¸ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                raise Exception("AP_NEWS_429_ERROR")
            else:
                print(f"âŒ AP HTTP ì˜¤ë¥˜: {e}")
                return []
        except Exception as e:
            if "AP_NEWS_429_ERROR" in str(e):
                raise
            print(f"âŒ AP ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    def extract_article_data(self, url: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ (429 ì—ëŸ¬ ì²˜ë¦¬ ë° ì§€ì—° ì‹œê°„ í¬í•¨)"""
        try:
            # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ ì¶”ê°€
            time.sleep(self.request_delay)
            
            response = requests.get(url, headers=self.headers, timeout=10)
            
            # 429 ì—ëŸ¬ ì²´í¬
            if response.status_code == 429:
                print(f"âš ï¸ {self.source_name}: 429 Too Many Requests ë°œìƒ (ê¸°ì‚¬ ì¶”ì¶œ)")
                raise requests.exceptions.HTTPError("429 Too Many Requests")
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('h1') or soup.find('title')
            title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
            
            # ë‚´ìš© ì¶”ì¶œ
            content_elems = soup.find_all('p')
            content = ' '.join([p.get_text(strip=True) for p in content_elems[:10]])
            
            return {
                'title': title,
                'content': content,
                'url': url,
                'source': self.source_name
            }
            
        except requests.exceptions.HTTPError as e:
            if "429" in str(e):
                print(f"âŒ {self.source_name}: ìš”ì²­ ì œí•œì— ê±¸ë ¸ìŠµë‹ˆë‹¤ ({url})")
                raise Exception("AP_NEWS_429_ERROR")
            else:
                print(f"âŒ AP HTTP ì˜¤ë¥˜ ({url}): {e}")
                return None
        except Exception as e:
            if "AP_NEWS_429_ERROR" in str(e):
                raise
            print(f"âŒ AP ê¸°ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
            return None
    def crawl(self, max_articles: int = 10) -> List[Dict[str, Any]]:
        """AP News í¬ë¡¤ë§ ì‹¤í–‰ (429 ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        print(f"ğŸ”„ {self.source_name} í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            # ë§í¬ ì¶”ì¶œ
            links = self.extract_article_links(max_articles)
            if not links:
                print(f"âŒ {self.source_name}: ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            articles = []
            for i, url in enumerate(links[:max_articles]):
                print(f"  ğŸ“° ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ {i+1}/{min(max_articles, len(links))}...")
                
                try:
                    article_data = self.extract_article_data(url)
                    if article_data:
                        # ë¶„ë¥˜ ì •ë³´ ì¶”ê°€
                        full_text = article_data['title'] + " " + article_data['content']
                        article_data['category'] = self.categorize_article(
                            article_data['title'], article_data['content']
                        )
                        article_data['countries'] = self.extract_countries(full_text)
                        article_data['is_incident'] = self.is_incident(full_text)
                        articles.append(article_data)
                except Exception as e:
                    if "AP_NEWS_429_ERROR" in str(e):
                        print(f"âš ï¸ {self.source_name}: 429 ì—ëŸ¬ë¡œ ì¸í•´ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        raise Exception("AP_NEWS_429_ERROR")
                    else:
                        print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
            
            print(f"âœ… {self.source_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            return articles
            
        except Exception as e:
            if "AP_NEWS_429_ERROR" in str(e):
                raise
            print(f"âŒ {self.source_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
