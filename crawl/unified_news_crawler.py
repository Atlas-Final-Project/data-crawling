# -*- coding: utf-8 -*-
"""
í†µí•© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
"""

from datetime import datetime
from collections import Counter
from typing import List, Dict, Any
from .bbc_news_crawler import BBCNewsCrawler
from .fox_news_crawler import FoxNewsCrawler
from .ap_news_crawler import APNewsCrawler


class UnifiedNewsCrawler:
    """í†µí•© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_file: str = "crawler_config.json"):
        """
        í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            config_file (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config_file = config_file
        self.crawlers = {
            'bbc': BBCNewsCrawler(config_file),
            'fox': FoxNewsCrawler(config_file),
            'ap': APNewsCrawler(config_file)
        }
    
    def crawl_all(self, max_articles_per_source: int = 10) -> Dict[str, List[Dict[str, Any]]]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸš€ í†µí•© ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        print("="*60)
        
        results = {}
        all_articles = []
        
        for source_name, crawler in self.crawlers.items():
            try:
                articles = crawler.crawl(max_articles_per_source)
                results[source_name] = articles
                all_articles.extend(articles)
                print()
            except Exception as e:
                print(f"âŒ {source_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                results[source_name] = []
        
        # í†µí•© ê²°ê³¼ ì €ì¥
        if all_articles:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"unified_news_{timestamp}.json"
            
            # ì²« ë²ˆì§¸ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ ì €ì¥
            if self.crawlers:
                first_crawler = next(iter(self.crawlers.values()))
                first_crawler.save_to_json(all_articles, filename)
        
        self.print_summary(results, all_articles)
        return results
    
    def crawl_single(self, source: str, max_articles: int = 10) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§"""
        if source not in self.crawlers:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤: {source}")
            print(f"ì§€ì› ì†ŒìŠ¤: {list(self.crawlers.keys())}")
            return []
        
        crawler = self.crawlers[source]
        return crawler.crawl(max_articles)
    
    def print_summary(self, results: Dict[str, List[Dict[str, Any]]], all_articles: List[Dict[str, Any]]):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        for source, articles in results.items():
            print(f"ğŸ“° {source.upper()}: {len(articles)}ê°œ ê¸°ì‚¬")
        
        print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(all_articles)}ê°œ")
        
        if all_articles:
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            categories = Counter(article.get('category', 'Unknown') for article in all_articles)
            print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            for category, count in categories.most_common():
                print(f"  â€¢ {category}: {count}ê°œ")
            
            # êµ­ê°€ë³„ ì–¸ê¸‰ ë¹ˆë„
            all_countries = []
            for article in all_articles:
                all_countries.extend(article.get('countries', []))
            country_counts = Counter(all_countries)
            
            print(f"\nğŸŒ ì£¼ìš” ì–¸ê¸‰ êµ­ê°€ (ìƒìœ„ 10ê°œ):")
            for country, count in country_counts.most_common(10):
                if country != "Unknown":
                    print(f"  â€¢ {country}: {count}íšŒ")
            
            # ì‚¬ê±´ì‚¬ê³  ê¸°ì‚¬ ìˆ˜
            incident_count = sum(1 for article in all_articles if article.get('is_incident', False))
            print(f"\nğŸš¨ ì‚¬ê±´Â·ì‚¬ê³  ê´€ë ¨ ê¸°ì‚¬: {incident_count}ê°œ")
        
        print("="*60)
