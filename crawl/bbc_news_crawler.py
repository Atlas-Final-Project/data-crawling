# -*- coding: utf-8 -*-
"""
BBC ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
BBC RSS í”¼ë“œë¥¼ í†µí•´ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§
"""

import feedparser
from newspaper import Article
from typing import List, Dict, Optional, Any
from .base_news_crawler import BaseNewsCrawler


class BBCNewsCrawler(BaseNewsCrawler):
    """BBC ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_file: str = "crawler_config.json"):
        super().__init__(config_file)
        self.source_name = "BBC News"
        self.rss_url = self.config.get("sources", {}).get("bbc", {}).get("rss_url", "")
    
    def get_rss_feed(self) -> Optional[feedparser.FeedParserDict]:
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            feed = feedparser.parse(self.rss_url)
            if feed.bozo:
                print(f"âš ï¸ RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            return feed
        except Exception as e:
            print(f"âŒ RSS í”¼ë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_article_content(self, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        try:
            article = Article(url)
            article.download()
            article.parse()
            
            return {
                'title': article.title,
                'content': article.text,
                'authors': ', '.join(article.authors) if article.authors else 'Unknown',
                'publish_date': str(article.publish_date) if article.publish_date else 'Unknown',
                'url': url,
                'summary': article.summary if hasattr(article, 'summary') else ''
            }
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
            return None
    
    def crawl(self, max_articles: int = 10) -> List[Dict[str, Any]]:
        """BBC ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ”„ {self.source_name} í¬ë¡¤ë§ ì‹œì‘...")
        
        feed = self.get_rss_feed()
        if not feed:
            return []
        
        articles = []
        for i, entry in enumerate(feed.entries[:max_articles]):
            print(f"  ğŸ“° ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ {i+1}/{min(max_articles, len(feed.entries))}...")
            
            article_data = self.extract_article_content(entry.link)
            if article_data:
                # RSS ì •ë³´ ì¶”ê°€
                article_data['rss_title'] = getattr(entry, 'title', '')
                article_data['rss_published'] = getattr(entry, 'published', '')
                article_data['rss_summary'] = getattr(entry, 'summary', '')
                article_data['source'] = self.source_name
                
                # ë¶„ë¥˜ ì •ë³´ ì¶”ê°€
                article_data['category'] = self.categorize_article(
                    article_data['title'], article_data['content']
                )
                article_data['countries'] = self.extract_countries(
                    article_data['title'] + " " + article_data['content']
                )
                article_data['is_incident'] = self.is_incident(
                    article_data['title'] + " " + article_data['content']
                )
                
                articles.append(article_data)
        
        print(f"âœ… {self.source_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
        return articles
