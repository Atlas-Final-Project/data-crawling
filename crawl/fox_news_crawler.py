# -*- coding: utf-8 -*-
"""
Fox News í¬ë¡¤ëŸ¬
"""

import feedparser
from typing import List, Dict, Any
from .base_news_crawler import BaseNewsCrawler


class FoxNewsCrawler(BaseNewsCrawler):
    """Fox News í¬ë¡¤ëŸ¬"""
    
    def __init__(self, config_file: str = "crawler_config.json"):
        super().__init__(config_file)
        self.source_name = "Fox News"
        self.rss_urls = self.config.get("sources", {}).get("fox", {}).get("rss_urls", [])
    
    def crawl_rss_feed(self, rss_url: str, max_articles: int = 10) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ RSS í”¼ë“œ í¬ë¡¤ë§"""
        try:
            feed = feedparser.parse(rss_url)
            articles = []
            
            for entry in feed.entries[:max_articles]:
                article_data = {
                    'title': getattr(entry, 'title', ''),
                    'content': getattr(entry, 'summary', ''),
                    'url': getattr(entry, 'link', ''),
                    'published': getattr(entry, 'published', ''),
                    'source': self.source_name,
                    'rss_url': rss_url
                }
                
                # ë¶„ë¥˜ ì •ë³´ ì¶”ê°€
                full_text = article_data['title'] + " " + article_data['content']
                article_data['category'] = self.categorize_article(
                    article_data['title'], article_data['content']
                )
                article_data['countries'] = self.extract_countries(full_text)
                article_data['is_incident'] = self.is_incident(full_text)
                
                articles.append(article_data)
            
            return articles
            
        except Exception as e:
            print(f"âŒ Fox RSS í¬ë¡¤ë§ ì˜¤ë¥˜ ({rss_url}): {e}")
            return []
    
    def crawl(self, max_articles: int = 10) -> List[Dict[str, Any]]:
        """Fox News í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"ğŸ”„ {self.source_name} í¬ë¡¤ë§ ì‹œì‘...")
        
        all_articles = []
        for rss_url in self.rss_urls:
            print(f"  ğŸ“¡ RSS í”¼ë“œ ì²˜ë¦¬: {rss_url}")
            articles = self.crawl_rss_feed(rss_url, max_articles // len(self.rss_urls))
            all_articles.extend(articles)
        
        print(f"âœ… {self.source_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(all_articles)}ê°œ ê¸°ì‚¬")
        return all_articles
