"""
í†µí•© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹¤í–‰ íŒŒì¼ (ìë™ ìŠ¤ì¼€ì¤„ë§ í¬í•¨)
crawl íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  MongoDBì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import time
import logging
import schedule
from datetime import datetime, timedelta
from crawl import UnifiedNewsCrawler
from db import MongoDBManager
from news_classification import NewsLocationExtractor


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# AP News ì¬ì‹œë„ ê´€ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
ap_retry_time = None

def save_articles_to_db(articles, db_manager, location_extractor):
    """ê¸°ì‚¬ë¥¼ MongoDBì— ì €ì¥ (ì œëª©, ë°œí–‰ì¼, ë³¸ë¬¸, ì†ŒìŠ¤, ìœ„ì¹˜ ì •ë³´ í¬í•¨)"""
    if not articles:
        logging.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    try:
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì œëª©ì„ ê¸°ì¤€ìœ¼ë¡œ upsert
        inserted_count = 0
        updated_count = 0
        
        for article in articles:
            # ë°œí–‰ì¼ ì •ê·œí™” (BaseNewsCrawlerì˜ normalize_dateì™€ ë™ì¼í•œ ë¡œì§)
            raw_published = (article.get('rss_published', '') or 
                        article.get('published', '') or
                        article.get('publish_date', '')) or article.get('pubDate', '')
            
            if not raw_published or raw_published == 'Unknown':
                published = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            else:
                try:
                    from dateutil import parser as date_parser
                    parsed_date = date_parser.parse(str(raw_published))
                    published = parsed_date.strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    published = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
            title = article.get('title', '')
            content = article.get('content', '')
            full_text = f"{title} {content}"
            
            locations = []
            if full_text.strip():
                try:
                    extracted_locations = location_extractor.extract_locations(
                        text=full_text, 
                        min_score=0.9, 
                        min_length=2
                    )
                    locations = [loc['word'] for loc in extracted_locations]
                    if locations:
                        logging.info(f"ìœ„ì¹˜ ì¶”ì¶œ ì™„ë£Œ: {locations} - {title[:50]}...")
                except Exception as e:
                    logging.warning(f"ìœ„ì¹˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e} - {title[:50]}...")
                    locations = []
            
            # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ê°„ì†Œí™”ëœ ê¸°ì‚¬ ë°ì´í„° ìƒì„± (êµ­ê°€, ì¹´í…Œê³ ë¦¬, ìœ„ì¹˜ í¬í•¨)
            simplified_article = {
                "title": title,
                "published": published,
                "content": content,
                "source": article.get('source', 'Unknown'),
                "category": article.get('category', 'General'),
                "countries": article.get('countries', ['Unknown']),
                "locations": locations,  # ìƒˆë¡œ ì¶”ê°€ëœ ìœ„ì¹˜ ì •ë³´
                "crawled_at": datetime.now().isoformat()
            }
            # ì œëª©ê³¼ ì†ŒìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸ í›„ upsert (ë” ì •í™•í•œ ì¤‘ë³µ ì²˜ë¦¬)
            filter_dict = {
                "title": simplified_article.get("title", ""),
                "source": simplified_article.get("source", "Unknown")
            }
            result = db_manager.upsert("news", filter_dict, simplified_article)
            
            if result == "updated":
                updated_count += 1
            else:
                inserted_count += 1
        
        logging.info(f"DB ì €ì¥ ì™„ë£Œ - ìƒˆë¡œ ì‚½ì…: {inserted_count}ê°œ, ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
        return inserted_count + updated_count
        
    except Exception as e:
        logging.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0

def crawl_and_save():
    """í¬ë¡¤ë§ ì‹¤í–‰ ë° DB ì €ì¥"""
    global ap_retry_time
    logging.info("ğŸš€ ìë™ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        # í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        unified_crawler = UnifiedNewsCrawler()
        
        # ìœ„ì¹˜ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        location_extractor = NewsLocationExtractor()
        logging.info("ğŸ“ ìœ„ì¹˜ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # MongoDB ì—°ê²° (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        # MONGO_URIì™€ DATABASE_NAMEì„ .env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜
        # ì§ì ‘ ì§€ì • (ë°ì´í„°ë² ì´ìŠ¤ëª…: Atlas)
        db_manager = MongoDBManager(
            connection_string=os.getenv("MONGO_URI"),
            database_name="Atlas"
        )
        
        # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ (ê¸°ì‚¬ë¥¼ ìµœëŒ€í•œ ë§ì´ ìˆ˜ì§‘)
        max_articles_per_source = 50  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 50ê°œ ê¸°ì‚¬
        
        logging.info("ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œì‘...")
        all_articles = []
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ ê°œë³„ í¬ë¡¤ë§
        sources = ['bbc', 'fox', 'ap']
        for source in sources:
            try:
                # AP Newsì˜ ê²½ìš° ì¬ì‹œë„ ì‹œê°„ ì²´í¬
                if source == 'ap' and ap_retry_time:
                    current_time = datetime.now()
                    if current_time < ap_retry_time:
                        remaining_minutes = int((ap_retry_time - current_time).total_seconds() / 60)
                        logging.info(f"AP News ì¬ì‹œë„ ëŒ€ê¸° ì¤‘... ë‚¨ì€ ì‹œê°„: {remaining_minutes}ë¶„")
                        continue
                    else:
                        # ì¬ì‹œë„ ì‹œê°„ì´ ì§€ë‚˜ë©´ ì´ˆê¸°í™”
                        ap_retry_time = None
                        logging.info("AP News ì¬ì‹œë„ ì‹œê°„ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¬ê°œí•©ë‹ˆë‹¤.")
                
                logging.info(f"{source.upper()} í¬ë¡¤ë§ ì‹œì‘...")
                articles = unified_crawler.crawl_single(source, max_articles_per_source)
                all_articles.extend(articles)
                logging.info(f"{source.upper()} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
                
            except Exception as e:
                error_msg = str(e)
                if source == 'ap' and "AP_NEWS_429_ERROR" in error_msg:
                    # AP News 429 ì—ëŸ¬ ì²˜ë¦¬
                    ap_retry_time = datetime.now() + timedelta(minutes=40)
                    logging.warning(f"âš ï¸ AP News 429 ì—ëŸ¬ ë°œìƒ. 40ë¶„ í›„ ì¬ì‹œë„: {ap_retry_time.strftime('%Y-%m-%d %H:%M:%S')}")
                else:
                    logging.error(f"{source.upper()} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ DBì— ì €ì¥
        if all_articles:
            saved_count = save_articles_to_db(all_articles, db_manager, location_extractor)
            logging.info(f"âœ… ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘, {saved_count}ê°œ DB ì €ì¥ ì™„ë£Œ")
        else:
            logging.warning("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # DB ì—°ê²° ì¢…ë£Œ
        db_manager.close_connection()
    except Exception as e:
        logging.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ë°”ë¡œ 10ë¶„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§ ì‹œì‘"""
    global ap_retry_time
    print("ğŸ—ï¸ í†µí•© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ìë™ ëª¨ë“œ")
    print("=" * 50)
    print("ğŸ•’ 10ë¶„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ë°ì´í„°ë² ì´ìŠ¤: Atlas, ì»¬ë ‰ì…˜: news")
    print("AP News 429 ì—ëŸ¬ ì‹œ 40ë¶„ í›„ ìë™ ì¬ì‹œë„")
    print("ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("=" * 50)
    
    try:
        # ì²« ì‹¤í–‰
        crawl_and_save()
        
        # 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ ìŠ¤ì¼€ì¤„ ì„¤ì •
        schedule.every(10).minutes.do(crawl_and_save)
        
        print(f"\nâ° ë‹¤ìŒ í¬ë¡¤ë§: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ì´í›„ 10ë¶„ë§ˆë‹¤")
        if ap_retry_time:
            print(f"ğŸ“ AP News ì¬ì‹œë„ ì˜ˆì •: {ap_retry_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...")
        
        # ìŠ¤ì¼€ì¤„ ì‹¤í–‰
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
            
            # AP News ìƒíƒœ ì£¼ê¸°ì  ì¶œë ¥ (5ë¶„ë§ˆë‹¤)
            if ap_retry_time and datetime.now().minute % 5 == 0:
                remaining_minutes = int((ap_retry_time - datetime.now()).total_seconds() / 60)
                if remaining_minutes > 0:
                    print(f"ğŸ“ AP News ì¬ì‹œë„ê¹Œì§€ ë‚¨ì€ ì‹œê°„: {remaining_minutes}ë¶„")
            
    except KeyboardInterrupt:
        print("\nâ­• ìë™ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logging.info("ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"ë©”ì¸ í•¨ìˆ˜ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
